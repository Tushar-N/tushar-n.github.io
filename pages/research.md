# Publications
---

<table class="researchtable">
<tbody>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/plm.png"> </td>
<td markdown="span">
**PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding**  
Jang Hyun Cho\*, Andrea Madotto\*, Effrosyni Mavroudi\*, Triantafyllos Afouras\*, <ins>Tushar Nagarajan</ins>\*, Muhammad Maaz\*, Yale Song\*, Tengyu Ma\*, Shuming Hu\*<span class="more-text">, Suyog Jain, Miguel Martin, Huiyu Wang, Hanoona Rasheed, Peize Sun, Po-Yao Huang, Daniel Bolya, Nikhila Ravi, Shashank Jain, Tammy Stark, Shane Moon, Babak Damavandi, Vivian Lee, Andrew Westbury, Salman Khan, Philipp Kr√§henb√ºhl, Piotr Doll√°r, Lorenzo Torresani, Kristen Grauman, Christoph Feichtenhofer</span>
<button class="toggle-more" onclick="toggleMore(this)"> (+20 authors) </button>
<button class="toggle-less" onclick="toggleLess(this)" style="display:none;"> (collapse) </button>  
NeurIPS 2025 (**Spotlight**)  
**[[paper]](https://arxiv.org/abs/2504.13180)
[[code]](https://github.com/facebookresearch/perception_models?tab=readme-ov-file#perception-language-model-plm)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/switch_a_view.png"> </td>
<td markdown="span">
**Switch-a-View: Few-Shot View Selection Learned from Edited Videos**  
Sagnik Majumder, <ins>Tushar Nagarajan</ins>, Ziad Al-Halah, Kristen Grauman  
ICCV 2025  
**[[paper]](https://arxiv.org/abs/2412.18386)
[[project]](https://vision.cs.utexas.edu/projects/switch_a_view/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/egoadapt.png"> </td>
<td markdown="span">
**EgoAdapt: A Joint Distillation and Policy Learning Framework for Efficient Multisensory Egocentric Perception**  
Sanjoy Chowdhury, Subrata Biswas, Sayan Nag, <ins>Tushar Nagarajan</ins>, Calvin Murdock, Ishwarya Ananthabhotla, Yijun Qian, Vamsi Krishna Ithapu, Dinesh Manocha, Ruohan Gao  
ICCV 2025  
**[[paper]](https://arxiv.org/abs/2506.21080)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/vited.png"> </td>
<td markdown="span">
**VITED: Video Temporal Evidence Distillation**  
Yujie Lu, Yale Song, Lorenzo Torresani, William Wang, <ins>Tushar Nagarajan</ins>  
CVPR 2025  
**[[paper]](https://arxiv.org/abs/2503.12855)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/bimba.png"> </td>
<td markdown="span">
**BIMBA: Selective-Scan Compression for Long-Range Video Question Answering**  
Md Mohaiminul Islam, <ins>Tushar Nagarajan</ins>, Huiyu Wang, Gedas Bertasius, Lorenzo Torresani  
CVPR 2025  
üèÜ 1st place: EgoSchema Challenge (CVPR 2025)  
**[[paper]](https://arxiv.org/abs/2503.09590)
[[project]](https://sites.google.com/view/bimba-mllm)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/viewpoint_weaksup.png"> </td>
<td markdown="span">
**Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos**  
Sagnik Majumder, <ins>Tushar Nagarajan</ins>, Ziad Al-Halah, Reina Pradhan, Kristen Grauman  
CVPR 2025 (**Highlight**)  
**[[paper]](https://arxiv.org/abs/2411.08753)
[[project]](https://vision.cs.utexas.edu/projects/which-view-shows-it-best/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/expertaf.png"> </td>
<td markdown="span">
**ExpertAF: Expert Actionable Feedback from Video**  
Kumar Ashutosh, <ins>Tushar Nagarajan</ins>, Georgios Pavlakos, Kris Kitani, Kristen Grauman  
CVPR 2025  
**[[paper]](https://arxiv.org/abs/2408.00672)
[[project]](https://vision.cs.utexas.edu/projects/ExpertAF/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/vedit.png"> </td>
<td markdown="span">
**VEDIT: Latent Prediction Architecture For Procedural Video Representation Learning**  
Han Lin, <ins>Tushar Nagarajan</ins>, Nicolas Ballas, Mido Assran, Mojtaba Komeili, Mohit Bansal, Koustuv Sinha  
ICLR 2025  
**[[paper]](https://arxiv.org/abs/2410.03478)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/uitl_mm_llm.png"> </td>
<td markdown="span">
**User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance**  
Mrinal Verghese, Brian Chen, Hamid Eghbalzadeh, <ins>Tushar Nagarajan</ins>, Ruta Desai  
WACV 2025 (**Oral**)  
**[[paper]](https://www.arxiv.org/abs/2408.03160)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/anymal.png"> </td>
<td markdown="span">
**AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model**  
Seungwhan Moon\*, Andrea Madotto\*, Zhaojiang Lin\*, <ins>Tushar Nagarajan</ins>\*, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, Kavya Srinet, Babak Damavandi, Anuj Kumar  
EMNLP 2024 (Industry Track) (* equal contribution)  
**[[paper]](https://arxiv.org/abs/2309.16058)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/amego.png"> </td>
<td markdown="span">
**AMEGO: Active Memory from long EGOcentric videos**  
Gabriele Goletto, <ins>Tushar Nagarajan</ins>, Giuseppe Averta, Dima Damen  
ECCV 2024  
**[[paper]](https://arxiv.org/abs/2409.10917)
[[project]](https://gabrielegoletto.github.io/AMEGO/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/vidassist.png"> </td>
<td markdown="span">
**Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in Instructional Videos**  
Md Mohaiminul Islam, <ins>Tushar Nagarajan</ins>, Huiyu Wang, Fu-Jen Chu, Kris Kitani, Gedas Bertasius, Xitong Yang  
ECCV 2024 (**Oral**)  
**[[paper]](https://arxiv.org/abs/2409.20557)
[[project]](https://sites.google.com/view/vidassist)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/stepdiff.png"> </td>
<td markdown="span">
**Step Differences in Instructional Video**  
<ins>Tushar Nagarajan</ins>, Lorenzo Torresani  
CVPR 2024  
**[[paper]](https://arxiv.org/abs/2404.16222)
[[data/code]](https://github.com/facebookresearch/stepdiff/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/video_detours.png"> </td>
<td markdown="span">
**Detours for Navigating Instructional Videos**  
Kumar Ashutosh, Zihui Xue, <ins>Tushar Nagarajan</ins>, Kristen Grauman  
CVPR 2024 (**Highlight**)  
**[[paper]](https://arxiv.org/abs/2401.01823)
[[project]](https://vision.cs.utexas.edu/projects/detours/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/video_recap.png"> </td>
<td markdown="span">
**Video ReCap: Recursive Captioning of Hour-Long Videos**  
Md Mohaiminul Islam, Ngan Ho, Xitong Yang, <ins>Tushar Nagarajan</ins>, Lorenzo Torresani, Gedas Bertasius  
CVPR 2024  
üèÜ EgoVis Distinguished Paper Award (2025)  
**[[paper]](https://arxiv.org/abs/2402.13250)
[[code]](https://github.com/md-mohaiminul/VideoRecap)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/egoexo4d.png"> </td>
<td markdown="span">
**Ego-Exo4D: Understanding Skilled Human Activity from First-and Third-Person Perspectives**  
Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, <ins>Tushar Nagarajan</ins>\*<span class="more-text">, Triantafyllos Afouras\*, Kumar Ashutosh\*, Vijay Baiyya\*, Siddhant Bansal\*, Bikram Boote\*, Eugene Byrne\*, Zach Chavis\*, Joya Chen\*, Feng Cheng\*, Fu-Jen Chu\*, Sean Crane\*, Avijit Dasgupta\*, Jing Dong\*, Maria Escobar\*, Cristhian Forigua\*, Abrham Gebreselasie\*, Sanjay Haresh\*, Jing Huang\*, Md Mohaiminul Islam\*, Suyog Jain\*, Rawal Khirodkar\*, Devansh Kukreja\*, Kevin J Liang\*, Jia-Wei Liu\*, Sagnik Majumder\*, Yongsen Mao\*, Miguel Martin\*, Effrosyni Mavroudi\*, Francesco Ragusa\*, Santhosh Kumar Ramakrishnan\*, Luigi Seminara\*, Arjun Somayazulu\*, Yale Song\*, Shan Su\*, Zihui Xue\*, Edward Zhang\*, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo, Zhengyi Luo, Brighid Meredith, Austin Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chenan Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez, Gedas Bertasius, David Crandall, Dima Damen, Jakob Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C. V. Jawahar, Richard Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, Michael Wray</span>
<button class="toggle-more" onclick="toggleMore(this)"> (+95 authors) </button>
<button class="toggle-less" onclick="toggleLess(this)" style="display:none;"> (collapse) </button>  
CVPR 2024 (**Oral**)  
**[[paper]](https://arxiv.org/abs/2311.18259)
[[project]](https://ego-exo4d-data.org/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/htstep.png"> </td>
<td markdown="span">
**HT-Step: Aligning Instructional Articles with How-To Videos**  
Triantafyllos Afouras, Effrosyni Mavroudi, <ins>Tushar Nagarajan</ins>, Huiyu Wang, Lorenzo Torresani  
NeurIPS 2023  
**[[paper]](https://openreview.net/forum?id=vv3cocNsEK)
[[data/code]](https://github.com/facebookresearch/htstep)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/goalstep.png"> </td>
<td markdown="span">
**Ego4D Goal-Step: Toward Hierarchical Understanding of Procedural Activities**  
Yale Song, Eugene Byrne, <ins>Tushar Nagarajan</ins>, Huiyu Wang, Miguel Martin, Lorenzo Torresani  
NeurIPS 2023  
üèÜ EgoVis Distinguished Paper Award (2025)  
**[[paper]](https://openreview.net/forum?id=3BxYAaovKr)
[[code]](https://github.com/facebookresearch/ego4d-goalstep)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/egodistill.png"> </td>
<td markdown="span">
**EgoDistill: Egocentric Head Motion Distillation for Efficient Video Understanding**  
Shuhan Tan, <ins>Tushar Nagarajan</ins>, Kristen Grauman  
NeurIPS 2023  
**[[paper]](https://arxiv.org/abs/2301.02217)
[[project]](https://vision.cs.utexas.edu/projects/egodistill/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/egoenv.png"> </td>
<td markdown="span">
**EgoEnv: Human-centric environment representations from egocentric video**  
<ins>Tushar Nagarajan</ins>, Santhosh K. Ramakrishnan, Ruta Desai, James Hillis, Kristen Grauman  
NeurIPS 2023 (**Oral**)  
**[[paper]](https://arxiv.org/abs/2207.11365)
[[project]](https://vision.cs.utexas.edu/projects/ego-env/)
[[code]](https://github.com/facebookresearch/ego-env)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/ego4d.png"> </td>
<td markdown="span">
**Ego4d: Around the world in 3,000 hours of egocentric video**  
Kristen Grauman, Andrew Westbury, <ins>Tushar Nagarajan</ins>\*<span class="more-text">, Eugene Byrne\*, Zachary Chavis\*, Antonino Furnari\*, Rohit Girdhar\*, Jackson Hamburger\*, Hao Jiang\*, Miao Liu\*, Xingyu Liu\*, Miguel Martin\*, Ilija Radosavovic\*, Santhosh Kumar Ramakrishnan\*, Fiona Ryan\*, Jayant Sharma\*, Michael Wray\*, Mengmeng Xu\*, Eric Zhongcong Xu\*, Chen Zhao\*, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, Jitendra Malik</span>
<button class="toggle-more" onclick="toggleMore(this)"> (+82 authors) </button>
<button class="toggle-less" onclick="toggleLess(this)" style="display:none;"> (collapse) </button>  
CVPR 2022 (**Oral**)  
TPAMI 2023 Invited article: Best Papers of CVPR  
**[[paper]](https://arxiv.org/abs/2110.07058)
[[project]](https://ego4d-data.org/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/epc.png"> </td>
<td markdown="span">
**Environment Predictive Coding for Visual Navigation**  
Santhosh K. Ramakrishnan, <ins>Tushar Nagarajan</ins>, Ziad Al-Halah, Kristen Grauman  
ICLR 2022  
**[[paper]](https://arxiv.org/abs/2102.02337)
[[project]](https://vision.cs.utexas.edu/projects/epc/)
[[code]](https://github.com/srama2512/EPC-SSL)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/activitycontext.png"> </td>
<td markdown="span">
**Shaping embodied agent behavior with activity-context priors from egocentric video**  
<ins>Tushar Nagarajan</ins>, Kristen Grauman  
NeurIPS 2021 (**Spotlight**)  
**[[paper]](https://arxiv.org/abs/2110.07692)
[[project]](https://vision.cs.utexas.edu/projects/ego-rewards/)
[[talk]](https://vision.cs.utexas.edu/projects/ego-rewards/media/teaser.mp4)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/egoexo.png"> </td>
<td markdown="span">
**Ego-Exo: Transferring Visual Representations from Third-person to First-person Videos**  
Yanghao Li, <ins>Tushar Nagarajan</ins>, Bo Xiong, Kristen Grauman  
CVPR 2021  
**[[paper]](https://arxiv.org/abs/2104.07905)
[[code]](https://github.com/facebookresearch/Ego-Exo)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/diffcausal.png"> </td>
<td markdown="span">
**Differentiable Causal Discovery Under Unmeasured Confounding**  
Rohit Bhattacharya, <ins>Tushar Nagarajan</ins>, Daniel Malinsky, Ilya Shpitser  
AISTATS 2021  
**[[paper]](https://arxiv.org/abs/2010.06978)
[[code]](https://gitlab.com/rbhatta8/dcd)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/intexp.png"> </td>
<td markdown="span">
**Learning Affordance Landscapes for Interaction Exploration in 3D Environments**  
<ins>Tushar Nagarajan</ins>, Kristen Grauman  
NeurIPS 2020 (**Spotlight**)  
**[[paper]](https://arxiv.org/abs/2008.09241)
[[project]](https://vision.cs.utexas.edu/projects/interaction-exploration/)
[[talk]](https://vision.cs.utexas.edu/projects/interaction-exploration/media/spotlight.mp4)
[[code]](https://github.com/facebookresearch/interaction-exploration)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/egotopo.png"> </td>
<td markdown="span">
**Ego-Topo: Environment Affordances from Egocentric Video**  
<ins>Tushar Nagarajan</ins>, Yanghao Li, Christoph Feichtenhofer, Kristen Grauman  
CVPR 2020 (**Oral**)  
**[[paper]](https://arxiv.org/abs/2001.04583)
[[project]](https://vision.cs.utexas.edu/projects/ego-topo/)
[[talk]](https://www.youtube.com/watch?v=YTx4co3AIDY)
[[code]](https://github.com/facebookresearch/ego-topo)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/hotspots.png"> </td>
<td markdown="span">
**Grounded Human-Object Interaction Hotspots from Video**  
<ins>Tushar Nagarajan</ins>, Christoph Feichtenhofer, Kristen Grauman  
ICCV 2019  
**[[paper]](https://arxiv.org/abs/1812.04558)
[[project]](https://vision.cs.utexas.edu/projects/interaction-hotspots/)
[[code]](https://github.com/Tushar-N/interaction-hotspots)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/attrop.png"> </td>
<td markdown="span">
**Attributes as Operators: Factorizing Unseen Attribute-Object Compositions**  
<ins>Tushar Nagarajan</ins>, Kristen Grauman  
ECCV 2018  
**[[paper]](https://arxiv.org/abs/1803.09851)
[[code]](https://github.com/Tushar-N/attributes-as-operators)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/blockdrop.png"> </td>
<td markdown="span">
**BlockDrop: Dynamic Inference Paths in Residual Networks**  
Zuxuan Wu\*, <ins>Tushar Nagarajan</ins>\*, Abhishek Kumar, Steven Rennie, Larry S. Davis, Kristen Grauman, Rogerio Feris  
CVPR 2018 (**Spotlight**) (* equal contribution)  
**[[paper]](https://arxiv.org/abs/1711.08393)
[[code]](https://github.com/Tushar-N/blockdrop)
[[talk]](https://www.youtube.com/embed/sIkUzmgUaxc?start=2931&end=3172)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/candis.png"> </td>
<td markdown="span">
**CANDiS: Coupled & Attention-Driven Neural Distant Supervision**  
<ins>Tushar Nagarajan</ins>, Sharmistha Jat, Partha Talukdar  
ACL 2017 (Workshop)  
**[[paper]](https://arxiv.org/abs/1710.09942)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/amplm.png"> </td>
<td markdown="span">
**Computational antimicrobial peptide design and evaluation against multidrug-resistant clinical isolates of bacteria**  
Deepesh Nagarajan, <ins>Tushar Nagarajan</ins>, Natasha Roy, Omkar Kulkarni, Sathyabaarathi Ravichandran, Madhulika Mishra, Dipshikha Chakravortty, Nagasuma Chandra  
JBC 2018  
**[[paper]](https://www.jbc.org/content/early/2017/12/19/jbc.M117.805499.full.pdf)
[[code]](https://github.com/Tushar-N/amp-lm)**
</td>
</tr>

</tbody>
</table>