# Publications
---

<table class="researchtable">
<tbody>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/vited.png"> </td>
<td markdown="span">
**VITED: Video Temporal Evidence Distillation**  
Yujie Lu, Yale Song, Lorenzo Torresani, William Wang, <ins>Tushar Nagarajan</ins>  
CVPR 2025  
**[[paper]](https://arxiv.org/abs/2503.12855)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/bimba.png"> </td>
<td markdown="span">
**BIMBA: Selective-Scan Compression for Long-Range Video Question Answering**  
Md Mohaiminul Islam, <ins>Tushar Nagarajan</ins>, Huiyu Wang, Gedas Bertasius, Lorenzo Torresani  
CVPR 2025  
**[[paper]](https://arxiv.org/abs/2503.09590)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/viewpoint_weaksup.png"> </td>
<td markdown="span">
**Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos**  
Sagnik Majumder, <ins>Tushar Nagarajan</ins>, Ziad Al-Halah, Reina Pradhan, Kristen Grauman  
CVPR 2025  
**[[paper]](https://arxiv.org/abs/2411.08753)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/expertaf.png"> </td>
<td markdown="span">
**ExpertAF: Expert Actionable Feedback from Video**  
Kumar Ashutosh, <ins>Tushar Nagarajan</ins>, Georgios Pavlakos, Kris Kitani, Kristen Grauman  
CVPR 2025  
**[[paper]](https://arxiv.org/abs/2408.00672)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/vedit.png"> </td>
<td markdown="span">
**VEDIT: Latent Prediction Architecture For Procedural Video Representation Learning**  
Han Lin, <ins>Tushar Nagarajan</ins>, Nicolas Ballas, Mido Assran, Mojtaba Komeili, Mohit Bansal, Koustuv Sinha  
ICLR 2025  
**[[paper]](https://arxiv.org/abs/2410.03478)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/anymal.png"> </td>
<td markdown="span">
**AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model**  
Seungwhan Moon\*, Andrea Madotto\*, Zhaojiang Lin\*, <ins>Tushar Nagarajan</ins>\*, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, Kavya Srinet, Babak Damavandi, Anuj Kumar  
EMNLP 2024 (Industry Track) (* equal contribution)  
**[[paper]](https://arxiv.org/abs/2309.16058)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/uitl_mm_llm.png"> </td>
<td markdown="span">
**User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance**  
Mrinal Verghese, Brian Chen, Hamid Eghbalzadeh, <ins>Tushar Nagarajan</ins>, Ruta Desai  
WACV 2025 (**Oral**)  
**[[paper]](https://www.arxiv.org/abs/2408.03160)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/amego.png"> </td>
<td markdown="span">
**AMEGO: Active Memory from long EGOcentric videos**  
Gabriele Goletto, <ins>Tushar Nagarajan</ins>, Giuseppe Averta, Dima Damen  
ECCV 2024  
**[[paper]](https://arxiv.org/abs/2409.10917)
[[project]](https://gabrielegoletto.github.io/AMEGO/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/vidassist.png"> </td>
<td markdown="span">
**Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in Instructional Videos**  
Md Mohaiminul Islam, <ins>Tushar Nagarajan</ins>, Huiyu Wang, Fu-Jen Chu, Kris Kitani, Gedas Bertasius, Xitong Yang  
ECCV 2024 (**Oral**)  
**[[paper]](https://arxiv.org/abs/2409.20557)
[[project]](https://sites.google.com/view/vidassist)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/stepdiff.png"> </td>
<td markdown="span">
**Step Differences in Instructional Video**  
<ins>Tushar Nagarajan</ins>, Lorenzo Torresani  
CVPR 2024  
**[[paper]](https://arxiv.org/abs/2404.16222)
[[data/code]](https://github.com/facebookresearch/stepdiff/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/video_detours.png"> </td>
<td markdown="span">
**Detours for Navigating Instructional Videos**  
Kumar Ashutosh, Zihui Xue, <ins>Tushar Nagarajan</ins>, Kristen Grauman  
CVPR 2024 (**Highlight**)  
**[[paper]](https://arxiv.org/abs/2401.01823)
[[project]](https://vision.cs.utexas.edu/projects/detours/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/video_recap.png"> </td>
<td markdown="span">
**Video ReCap: Recursive Captioning of Hour-Long Videos**  
Md Mohaiminul Islam, Ngan Ho, Xitong Yang, <ins>Tushar Nagarajan</ins>, Lorenzo Torresani, Gedas Bertasius  
CVPR 2024  
**[[paper]](https://arxiv.org/abs/2402.13250)
[[code]](https://github.com/md-mohaiminul/VideoRecap)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/egoexo4d.png"> </td>
<td markdown="span">
**Ego-Exo4D: Understanding Skilled Human Activity from First-and Third-Person Perspectives**  
Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, <ins>Tushar Nagarajan</ins>\*, et al.  
CVPR 2024 (**Oral**)  
**[[paper]](https://arxiv.org/abs/2311.18259)
[[project]](https://ego-exo4d-data.org/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/htstep.png"> </td>
<td markdown="span">
**HT-Step: Aligning Instructional Articles with How-To Videos**  
Triantafyllos Afouras, Effrosyni Mavroudi, <ins>Tushar Nagarajan</ins>, Huiyu Wang, Lorenzo Torresani  
NeurIPS 2023  
**[[paper]](https://openreview.net/forum?id=vv3cocNsEK)
[[data/code]](https://github.com/facebookresearch/htstep)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/goalstep.png"> </td>
<td markdown="span">
**Ego4D Goal-Step: Toward Hierarchical Understanding of Procedural Activities**  
Yale Song, Eugene Byrne, <ins>Tushar Nagarajan</ins>, Huiyu Wang, Miguel Martin, Lorenzo Torresani  
NeurIPS 2023  
**[[paper]](https://openreview.net/forum?id=3BxYAaovKr)
[[code]](https://github.com/facebookresearch/ego4d-goalstep)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/egodistill.png"> </td>
<td markdown="span">
**EgoDistill: Egocentric Head Motion Distillation for Efficient Video Understanding**  
Shuhan Tan, <ins>Tushar Nagarajan</ins>, Kristen Grauman  
NeurIPS 2023  
**[[paper]](https://arxiv.org/abs/2301.02217)
[[project]](https://vision.cs.utexas.edu/projects/egodistill/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/egoenv.png"> </td>
<td markdown="span">
**EgoEnv: Human-centric environment representations from egocentric video**  
<ins>Tushar Nagarajan</ins>, Santhosh K. Ramakrishnan, Ruta Desai, James Hillis, Kristen Grauman  
NeurIPS 2023 (**Oral**)  
**[[paper]](https://arxiv.org/abs/2207.11365)
[[project]](https://vision.cs.utexas.edu/projects/ego-env/)
[[code]](https://github.com/facebookresearch/ego-env)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/ego4d.png"> </td>
<td markdown="span">
**Ego4d: Around the world in 3,000 hours of egocentric video**  
Kristen Grauman, Andrew Westbury, <ins>Tushar Nagarajan</ins>\*, et al.  
CVPR 2022 (**Oral**)
TPAMI 2023 Invited article: Best Papers of CVPR    
**[[paper]](https://arxiv.org/abs/2110.07058)
[[project]](https://ego4d-data.org/)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/epc.png"> </td>
<td markdown="span">
**Environment Predictive Coding for Visual Navigation**  
Santhosh K. Ramakrishnan, <ins>Tushar Nagarajan</ins>, Ziad Al-Halah, Kristen Grauman  
ICLR 2022  
**[[paper]](https://arxiv.org/abs/2102.02337)
[[project]](https://vision.cs.utexas.edu/projects/epc/)
[[code]](https://github.com/srama2512/EPC-SSL)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/activitycontext.png"> </td>
<td markdown="span">
**Shaping embodied agent behavior with activity-context priors from egocentric video**  
<ins>Tushar Nagarajan</ins>, Kristen Grauman  
NeurIPS 2021 (**Spotlight**)  
**[[paper]](https://arxiv.org/abs/2110.07692)
[[project]](https://vision.cs.utexas.edu/projects/ego-rewards/)
[[talk]](https://vision.cs.utexas.edu/projects/ego-rewards/media/teaser.mp4)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/egoexo.png"> </td>
<td markdown="span">
**Ego-Exo: Transferring Visual Representations from Third-person to First-person Videos**  
Yanghao Li, <ins>Tushar Nagarajan</ins>, Bo Xiong, Kristen Grauman  
CVPR 2021  
**[[paper]](https://arxiv.org/abs/2104.07905)
[[code]](https://github.com/facebookresearch/Ego-Exo)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/diffcausal.png"> </td>
<td markdown="span">
**Differentiable Causal Discovery Under Unmeasured Confounding**  
Rohit Bhattacharya, <ins>Tushar Nagarajan</ins>, Daniel Malinsky, Ilya Shpitser  
AISTATS 2021  
**[[paper]](https://arxiv.org/abs/2010.06978)
[[code]](https://gitlab.com/rbhatta8/dcd)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/intexp.png"> </td>
<td markdown="span">
**Learning Affordance Landscapes for Interaction Exploration in 3D Environments**  
<ins>Tushar Nagarajan</ins>, Kristen Grauman  
NeurIPS 2020 (**Spotlight**)  
**[[paper]](https://arxiv.org/abs/2008.09241)
[[project]](https://vision.cs.utexas.edu/projects/interaction-exploration/)
[[talk]](https://vision.cs.utexas.edu/projects/interaction-exploration/media/spotlight.mp4)
[[code]](https://github.com/facebookresearch/interaction-exploration)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/egotopo.png"> </td>
<td markdown="span">
**Ego-Topo: Environment Affordances from Egocentric Video**  
<ins>Tushar Nagarajan</ins>, Yanghao Li, Christoph Feichtenhofer, Kristen Grauman  
CVPR 2020 (**Oral**)  
**[[paper]](https://arxiv.org/abs/2001.04583)
[[project]](https://vision.cs.utexas.edu/projects/ego-topo/)
[[talk]](https://www.youtube.com/watch?v=YTx4co3AIDY)
[[code]](https://github.com/facebookresearch/ego-topo)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/hotspots.png"> </td>
<td markdown="span">
**Grounded Human-Object Interaction Hotspots from Video**  
<ins>Tushar Nagarajan</ins>, Christoph Feichtenhofer, Kristen Grauman  
ICCV 2019  
**[[paper]](https://arxiv.org/abs/1812.04558)
[[project]](https://vision.cs.utexas.edu/projects/interaction-hotspots/)
[[code]](https://github.com/Tushar-N/interaction-hotspots)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/attrop.png"> </td>
<td markdown="span">
**Attributes as Operators: Factorizing Unseen Attribute-Object Compositions**  
<ins>Tushar Nagarajan</ins>, Kristen Grauman  
ECCV 2018  
**[[paper]](https://arxiv.org/abs/1803.09851)
[[code]](https://github.com/Tushar-N/attributes-as-operators)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/blockdrop.png"> </td>
<td markdown="span">
**BlockDrop: Dynamic Inference Paths in Residual Networks**  
Zuxuan Wu\*, <ins>Tushar Nagarajan</ins>\*, Abhishek Kumar, Steven Rennie, Larry S. Davis, Kristen Grauman, Rogerio Feris  
CVPR 2018 (**Spotlight**) (* equal contribution)  
**[[paper]](https://arxiv.org/abs/1711.08393)
[[code]](https://github.com/Tushar-N/blockdrop)
[[talk]](https://www.youtube.com/embed/sIkUzmgUaxc?start=2931&end=3172)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/candis.png"> </td>
<td markdown="span">
**CANDiS: Coupled & Attention-Driven Neural Distant Supervision**  
<ins>Tushar Nagarajan</ins>, Sharmistha Jat, Partha Talukdar  
ACL 2017 (Workshop)  
**[[paper]](https://arxiv.org/abs/1710.09942)**
</td>
</tr>

<tr>
<td class="img"> <img src="https://tushar-n.s3.amazonaws.com/thumbnails/amplm.png"> </td>
<td markdown="span">
**Computational antimicrobial peptide design and evaluation against multidrug-resistant clinical isolates of bacteria**  
Deepesh Nagarajan, <ins>Tushar Nagarajan</ins>, Natasha Roy, Omkar Kulkarni, Sathyabaarathi Ravichandran, Madhulika Mishra, Dipshikha Chakravortty, Nagasuma Chandra  
JBC 2018  
**[[paper]](https://www.jbc.org/content/early/2017/12/19/jbc.M117.805499.full.pdf)
[[code]](https://github.com/Tushar-N/amp-lm)**
</td>
</tr>

</tbody>
</table>